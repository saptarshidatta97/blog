<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Sign Language Digit Predictionwith MobileNet | Saptarshi Datta</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Sign Language Digit Predictionwith MobileNet" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the last post, we applied Transfer Learning in the VGG-16 Model with the Cat vs Dogs data set. However the application was minimal as we only changed the last output layer from a ‘softmax’ actiavted outpur to a ‘sigmoid’ activated output. Additionally, the VGG-16 model was already trained on the ImageNet Data, which originally had imaged of cats and dogs. We jist trained the last dense layer which predicted whether the image is og a cat or a dog." />
<meta property="og:description" content="In the last post, we applied Transfer Learning in the VGG-16 Model with the Cat vs Dogs data set. However the application was minimal as we only changed the last output layer from a ‘softmax’ actiavted outpur to a ‘sigmoid’ activated output. Additionally, the VGG-16 model was already trained on the ImageNet Data, which originally had imaged of cats and dogs. We jist trained the last dense layer which predicted whether the image is og a cat or a dog." />
<link rel="canonical" href="https://git.saptarshidatta.in/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html" />
<meta property="og:url" content="https://git.saptarshidatta.in/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html" />
<meta property="og:site_name" content="Saptarshi Datta" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-08T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Sign Language Digit Predictionwith MobileNet","dateModified":"2020-09-08T00:00:00-05:00","datePublished":"2020-09-08T00:00:00-05:00","description":"In the last post, we applied Transfer Learning in the VGG-16 Model with the Cat vs Dogs data set. However the application was minimal as we only changed the last output layer from a ‘softmax’ actiavted outpur to a ‘sigmoid’ activated output. Additionally, the VGG-16 model was already trained on the ImageNet Data, which originally had imaged of cats and dogs. We jist trained the last dense layer which predicted whether the image is og a cat or a dog.","mainEntityOfPage":{"@type":"WebPage","@id":"https://git.saptarshidatta.in/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html"},"@type":"BlogPosting","url":"https://git.saptarshidatta.in/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://git.saptarshidatta.in/feed.xml" title="Saptarshi Datta" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Saptarshi Datta</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Sign Language Digit Predictionwith MobileNet</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-08T00:00:00-05:00" itemprop="datePublished">
        Sep 8, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the last <a href="https://git.saptarshidatta.in/Transfer-Learning-with-VGG-16/">post</a>, we applied Transfer Learning in the VGG-16 Model with the Cat vs Dogs data set. However the application was minimal as we only changed the last output layer from a ‘softmax’ actiavted outpur to a ‘sigmoid’ activated output. Additionally, the VGG-16 model was already trained on the ImageNet Data, which originally had imaged of cats and dogs. We jist trained the last dense layer which predicted whether the image is og a cat or a dog.</p>

<p>However, in this exercise we shall again apply transfer learning to predict the Numeric Sign Language. We will be applying MobileNet Model and shall modify the model and then fine tune it to suit our requirements. But before that, let’s discuss a bit about the MobileNet Model.</p>

<h2 id="mobilenet-model">MobileNet Model</h2>
<p>MobileNets are a class of small, low-latency and low-power model that can be used for classification, detection, and other common tasks convolutional neural networks are good for. Because of their small size, these are considered great deep learning models to be used on mobile devices.</p>

<p>The MobileNet is about 17 MB in size and has just 4.2 million parameters as compared to the VGG-16 model which has a size of 534 MB and around 138 Million parameters.</p>

<p>However due to their smaller size and faster performance than other networks, MobileNets aren’t as accurate as the other large, resource-heavy models. However they still actually perform very well, with really only a relatively small reduction in accuracy.</p>

<p>Please go through the <a href="https://arxiv.org/pdf/1704.04861.pdf">MobileNet Paper</a> which elaborates further regarding the tradeoff between accuracy and size.</p>

<p>Having discussed the MobileNet model to some extent, let move ahead to other sections. Alright, let’s jump into the code!</p>

<h2 id="data-preparation">Data Preparation</h2>
<p>We have used the Sign Language Digits dataset from <a href="https://github.com/ardamavi/Sign-Language-Digits-Dataset">GitHub</a>. The data is located in corresponding folders ranging from 0-9, however we will use a script to divide the data into train, test and valid datasets.</p>

<p>`
os.chdir(‘/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset’)
if os.path.isdir(‘train/0/’) is False: 
    os.mkdir(‘train’)
    os.mkdir(‘valid’)
    os.mkdir(‘test’)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in range(0, 10):
    shutil.move(f'{i}', 'train')
    os.mkdir(f'valid/{i}')
    os.mkdir(f'test/{i}')

    valid_samples = random.sample(os.listdir(f'train/{i}'), 30)
    for j in valid_samples:
        shutil.move(f'train/{i}/{j}', f'valid/{i}')

    test_samples = random.sample(os.listdir(f'train/{i}'), 5)
    for k in test_samples:
        shutil.move(f'train/{i}/{k}', f'test/{i}')
</code></pre></div></div>

<p>os.chdir(‘../..’)
`</p>

<p>So what we are basically doing in the above script is at first checking whether a train folder already exists, if not, we are creating a train/test/valid folders. Then, we will move all the images corresponding to a particular class-folder from the main folder to the corresponding class-folder inside the train folder, and at the same time creating new class-folder inside the valid and test folders. Then we randomly move 30 and 5 images from each class-folder inside train folder to the corresponding class-folders inside valid and test folders that were created before.
We run the entire process in a loop iterating over class-folders ranging from 0-9.</p>

<p>Next, we preprocess the train, valid and test data in a fashion the MobileNet model expects(MobileNet expects images to be scales between [-1,1] rather than [0, 225]. We set the batch size to 10 and the target image size to (224, 224) since that is the image size that MobileNet expects. We set shuffle to False for the test batch, so that later we can plot our results on to a confusion Matrix.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=train_path, target_size=(224,224), batch_size=10)
valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=valid_path, target_size=(224,224), batch_size=10)
test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(
    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)
</code></pre></div></div>

<h2 id="fine-tuning-the-mobilenet-model">Fine Tuning the MobileNet Model</h2>

<p>We will import the MobileNet Model just as we imported the VGG-16 Model. Then we will drop the last 5 layers forom the model and add a dense layer with softmax activation predicting 10 classes ranging from 0-9. Later we freeze all the layers except the last 23 layers (A MobileNet Model has 88 Layers). The choice of the number 23 is based upon personal choice and some experimentation. It was found out that if we train the last 23 layers, we get some really good results.
Please note that it is a significant deviation from the last VGG-16 Model training, where we only trained the last output layer.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mobile</span> <span class="p">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">mobilenet</span><span class="p">.</span><span class="n">MobileNet</span><span class="p">()</span>
<span class="n">x</span> <span class="p">=</span> <span class="n">mobile</span><span class="p">.</span><span class="n">layers</span><span class="p">[-</span><span class="m">6</span><span class="p">].</span><span class="n">output</span>
<span class="n">predictions</span> <span class="p">=</span> <span class="n">Dense</span><span class="p">(</span><span class="m">10</span><span class="p">,</span> <span class="n">activation</span><span class="p">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">model</span> <span class="p">=</span> <span class="k">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">=</span><span class="n">mobile</span><span class="p">.</span><span class="n">input</span><span class="p">,</span> <span class="n">outputs</span><span class="p">=</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">for</span> <span class="n">layer</span> <span class="k">in</span> <span class="k">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[:-</span><span class="m">23</span><span class="p">]:</span>
    <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="p">=</span> <span class="nb">False</span>
</code></pre></div></div>

<p>Now, we will compile and train the model for 30 epochs and our model scores 100% valodation accuracy and 98.89% train accuracy. This shows taht our model is generalising well.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x=train_batches, steps_per_epoch=18, validation_data=valid_batches, validation_steps=3, epochs=30, verbose=2)
</code></pre></div></div>

<p>Here are the accuracies for the last 5 epochs:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 25/30
18/18 - 1s - loss: 0.0449 - accuracy: 0.9944 - val_loss: 0.0444 - val_accuracy: 1.0000
Epoch 26/30
18/18 - 2s - loss: 0.0510 - accuracy: 0.9944 - val_loss: 0.0346 - val_accuracy: 1.0000
Epoch 27/30
18/18 - 1s - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 1.0000
Epoch 28/30
18/18 - 2s - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000
Epoch 29/30
18/18 - 2s - loss: 0.0427 - accuracy: 0.9944 - val_loss: 0.0664 - val_accuracy: 0.9667
Epoch 30/30
18/18 - 1s - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0609 - val_accuracy: 1.0000
&lt;tensorflow.python.keras.callbacks.History at 0x7fd299fcbb00&gt;
</code></pre></div></div>

<h2 id="prediction-on-the-test-batch">Prediction on the Test Batch</h2>

<p>Now, let’s test the model on the test batch and plot the confusion matrix.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predictions = model.predict(x=test_batches, steps=5, verbose=0)
cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))
cm_plot_labels = ['0','1','2','3','4','5','6','7','8','9']
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')

Confusion matrix, without normalization
[[5 0 0 0 0 0 0 0 0 0]
 [0 5 0 0 0 0 0 0 0 0]
 [0 0 5 0 0 0 0 0 0 0]
 [0 0 0 5 0 0 0 0 0 0]
 [0 0 0 0 5 0 0 0 0 0]
 [0 0 0 0 0 5 0 0 0 0]
 [0 0 0 0 0 0 5 0 0 0]
 [0 0 0 0 0 0 0 4 1 0]
 [0 0 0 0 0 0 0 0 5 0]
 [0 0 0 0 0 0 0 0 0 5]]
</code></pre></div></div>

<p>Our model has performed excellent on the Test batch with only one error.
Please access the <a href="https://github.com/saptarshidatta96/Sign-Language-Digits-Prediction">GitHub</a> Repository to access the Python Notebook associated with this exercise.</p>

  </div><a class="u-url" href="/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Deep Learning and Computer Vision</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/saptarshidatta96" title="saptarshidatta96"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ds_saptarshi" title="ds_saptarshi"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
